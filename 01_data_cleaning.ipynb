{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Usage in Seattle, 2005-2020\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The data is the [Checkouts by Title (Physical Items)](https://data.seattle.gov/Community/Checkouts-By-Title-Physical-Items-/5src-czff) dataset from [Seattle Open Data](https://data.seattle.gov/) and was downloaded on December 15, 2020.\n",
    "\n",
    "This notebook is designed to load a downloaded CSV file, merge it with item-specific information, convert it to a time-series-ready DataFrame, and save that as a compressed pickle file.\n",
    "\n",
    "*Note: This dataset is updated weekly; the more data, the longer the load times will be.*\n",
    "\n",
    "In the future, it may be a good idea to look into adding [API](https://dev.socrata.com/foundry/data.seattle.gov/5src-czff) calls into the pipeline, so as to quickly and easily add on the additional weekly data.\n",
    "\n",
    "*Note: Any cell that uses the built-in magic command* `%%time` *takes a significant (or at least not insignificant) time to run.*\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dataframe packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# saving packages\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from functions.data_cleaning import status_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkout data\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "Since the data set is so large, I'll specify only the columns that I want in the DataFrame. This will effectively drop the following columns:\n",
    "- `ID`\n",
    "- `CheckoutYear`\n",
    "- `CallNumber`\n",
    "- `BibNumber`\n",
    "- `ItemBarcode`\n",
    "- `ItemType`\n",
    "    \n",
    "I want to note that the `ItemType` and `Collection` columns are very similar, but the code in the `Collection` column contains more information within the `category_group` column that I add onto the DataFrame using the `data_dictionary.csv` file ([see below](#Load-other-info-from-data-dictionary-and-merge-onto-checkouts-dataset)). More specifically, the `ItemType` code yields mostly \"Miscellaneous\" results, whereas the `Collection` code yields differentiates between \"Fiction\" and \"Nonfiction\", among others. This could be useful information later on, so I found it best to drop the `ItemType` column.\n",
    "\n",
    "Note: In the future, I may consider using this [dataset](https://data.seattle.gov/Community/Library-Collection-Inventory/6vkj-f5xf) to add on branch information (i.e. which branch an item was checked out from), although this data is rather limited (due to privacy concerns) and incomplete (only appears to be collected beginning in 2017). In order to do that I would need to use the `BibNumber` column.\n",
    "\n",
    "#### ⏰ Cell below takes ~14.5 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 20s, sys: 4min 39s, total: 9min\n",
      "Wall time: 14min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# columns to load\n",
    "usecols = ['Collection', 'ItemTitle', 'Subjects', 'CheckoutDateTime']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('data/Checkouts_By_Title__Physical_Items_.csv',\n",
    "#                  nrows=10000000,\n",
    "                 usecols=usecols)\n",
    "\n",
    "# rename columns to my preferred format\n",
    "df.columns = ['collection', 'title', 'subjects', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: While it only took about 25 seconds to load 10 million rows, it takes about 25 minutes to load 106.5 million rows with the same number (5) of columns. In my latest update, I brought it down to 4 columns, which decreased load time to 15.5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106534901, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>02/13/2008 07:38:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nanf</td>\n",
       "      <td>best baby shower book a complete guide for par...</td>\n",
       "      <td>Showers Parties</td>\n",
       "      <td>07/23/2008 02:53:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nyfic</td>\n",
       "      <td>Uglies</td>\n",
       "      <td>Fantasy, Teenage girls Fiction, Beauty Persona...</td>\n",
       "      <td>12/23/2009 04:20:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>napar</td>\n",
       "      <td>doula guide to birth secrets every pregnant wo...</td>\n",
       "      <td>Doulas, Childbirth</td>\n",
       "      <td>11/16/2010 12:04:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canf</td>\n",
       "      <td>Salmon a cookbook</td>\n",
       "      <td>Cookery Salmon</td>\n",
       "      <td>04/26/2009 01:29:00 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  collection                                              title  \\\n",
       "0      nadvd                                           Firewall   \n",
       "1       nanf  best baby shower book a complete guide for par...   \n",
       "2      nyfic                                             Uglies   \n",
       "3      napar  doula guide to birth secrets every pregnant wo...   \n",
       "4       canf                                  Salmon a cookbook   \n",
       "\n",
       "                                            subjects                    date  \n",
       "0  Kidnapping Drama, Video recordings for the hea...  02/13/2008 07:38:00 PM  \n",
       "1                                    Showers Parties  07/23/2008 02:53:00 PM  \n",
       "2  Fantasy, Teenage girls Fiction, Beauty Persona...  12/23/2009 04:20:00 PM  \n",
       "3                                 Doulas, Childbirth  11/16/2010 12:04:00 PM  \n",
       "4                                     Cookery Salmon  04/26/2009 01:29:00 PM  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~3 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 1min 9s, total: 1min 43s\n",
      "Wall time: 3min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "collection          0\n",
       "title          900912\n",
       "subjects      1649522\n",
       "date                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check for nan values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: Even checking for NaN values takes a significant amount of time with this many rows.*\n",
    "\n",
    "The most important columns (`collection` and `date`) have no NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collection    object\n",
       "title         object\n",
       "subjects      object\n",
       "date          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `date` column to datetime\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/13/2008 07:38:00 PM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at an example before conversion\n",
    "df.loc[0, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the format\n",
    "dt_format = '%m/%d/%Y %I:%M:%S %p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~7 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 27s, sys: 15.6 s, total: 6min 43s\n",
      "Wall time: 6min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.date(2008, 2, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert to datetime, dropping the hour-minute-second stamp using the `dt.date` attribute\n",
    "df['date'] = pd.to_datetime(df.date, format=dt_format).dt.date\n",
    "\n",
    "# confirm it worked\n",
    "df.loc[0, 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load other info from data dictionary and merge onto checkouts dataset\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>description</th>\n",
       "      <th>code_type</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>category_subgroup</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cazover</td>\n",
       "      <td>CA7-zine collection oversize</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caziner</td>\n",
       "      <td>CA7-zine collection reference</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cazval</td>\n",
       "      <td>CA7-zine collection valuable mat.</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nga</td>\n",
       "      <td>Northgate Branch</td>\n",
       "      <td>Location</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hip</td>\n",
       "      <td>High Point Branch</td>\n",
       "      <td>Location</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      code                        description       code_type format_group  \\\n",
       "0  cazover       CA7-zine collection oversize  ItemCollection        Print   \n",
       "1  caziner      CA7-zine collection reference  ItemCollection        Print   \n",
       "2   cazval  CA7-zine collection valuable mat.  ItemCollection        Print   \n",
       "3      nga                   Northgate Branch        Location          NaN   \n",
       "4      hip                  High Point Branch        Location          NaN   \n",
       "\n",
       "  format_subgroup category_group category_subgroup age_group  \n",
       "0            Book     Periodical               NaN     Adult  \n",
       "1            Book     Periodical               NaN     Adult  \n",
       "2            Book     Periodical               NaN     Adult  \n",
       "3             NaN            NaN               NaN       NaN  \n",
       "4             NaN            NaN               NaN       NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "dd = pd.read_csv('data/data_dictionary.csv')\n",
    "\n",
    "# rename columns to my preferred format\n",
    "dd.columns = ['code', 'description', 'code_type', 'format_group', 'format_subgroup', \n",
    "              'category_group', 'category_subgroup', 'age_group']\n",
    "\n",
    "# take a look\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                 object\n",
       "description          object\n",
       "code_type            object\n",
       "format_group         object\n",
       "format_subgroup      object\n",
       "category_group       object\n",
       "category_subgroup    object\n",
       "age_group            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "dd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will only be using information from codes whose type is \"ItemCollection\", I'll subset the data dictionary down to just those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to only collection codes\n",
    "dd = dd[dd.code_type == 'ItemCollection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                   0\n",
       "description            0\n",
       "code_type              0\n",
       "format_group           0\n",
       "format_subgroup       28\n",
       "category_group         2\n",
       "category_subgroup    391\n",
       "age_group              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nan values\n",
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, with the size of the eventual DataFrame in mind, I want to drop any unnecessary columns before merging, so I'll drop the following columns:\n",
    "- `description`, since that is superfluous information for this project\n",
    "- `code_type`, since that is superfluous information\n",
    "- `category_subgroup`, since that is mostly NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "dd.drop(columns=['description', 'code_type', 'category_subgroup'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to convert\n",
    "to_convert = ['format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "# convert to category datatype\n",
    "dd[to_convert] = dd[to_convert].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                 object\n",
       "format_group       category\n",
       "format_subgroup    category\n",
       "category_group     category\n",
       "age_group          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm new datatypes\n",
    "dd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~4 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1min 50s, total: 2min 51s\n",
      "Wall time: 4min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Marley me</td>\n",
       "      <td>Comedy films, Married people Drama, Philadelph...</td>\n",
       "      <td>2009-07-03</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Six feet under The complete fourth season</td>\n",
       "      <td>Video recordings for the hearing impaired, Pro...</td>\n",
       "      <td>2008-10-26</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Doctor Who The next doctor</td>\n",
       "      <td>London England Drama, Doctor Who Fictitious ch...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>School ties</td>\n",
       "      <td>Antisemitism Drama, Video recordings for the h...</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  collection                                      title  \\\n",
       "0      nadvd                                   Firewall   \n",
       "1      nadvd                                  Marley me   \n",
       "2      nadvd  Six feet under The complete fourth season   \n",
       "3      nadvd                 Doctor Who The next doctor   \n",
       "4      nadvd                                School ties   \n",
       "\n",
       "                                            subjects        date   code  \\\n",
       "0  Kidnapping Drama, Video recordings for the hea...  2008-02-13  nadvd   \n",
       "1  Comedy films, Married people Drama, Philadelph...  2009-07-03  nadvd   \n",
       "2  Video recordings for the hearing impaired, Pro...  2008-10-26  nadvd   \n",
       "3  London England Drama, Doctor Who Fictitious ch...  2010-11-10  nadvd   \n",
       "4  Antisemitism Drama, Video recordings for the h...  2008-12-28  nadvd   \n",
       "\n",
       "  format_group format_subgroup category_group age_group  \n",
       "0        Media      Video Disc        Fiction     Adult  \n",
       "1        Media      Video Disc        Fiction     Adult  \n",
       "2        Media      Video Disc        Fiction     Adult  \n",
       "3        Media      Video Disc        Fiction     Adult  \n",
       "4        Media      Video Disc        Fiction     Adult  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# merge checkouts dataframe with info from data dictionary\n",
    "df_merged = df.merge(dd, left_on='collection', right_on='code')\n",
    "\n",
    "# take a look\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns\n",
    "\n",
    "I can now drop the `collection` and `code` columns, since those are no longer necessary.\n",
    "\n",
    "*NOTE: Using the Pandas method `.drop()` was taking well over an hour, so I'm going to try to subset it below, to see if that works any faster.*\n",
    "\n",
    "#### ⏰ Cell below takes ~38.5 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 18min 6s, total: 20min 50s\n",
      "Wall time: 44min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# drop columns\n",
    "df_merged.drop(columns=['collection', 'code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set `date` column as index\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "I've commented out the below code because now *this* has begun to consistently crash the kernel or zsh shell.\n",
    "\n",
    "After thinking more about it, I believe I will end up grouping by the date to get raw numbers for each day (not only total checkouts, but total print checkouts and fiction checkouts, etc.), which can be done before setting the `date` column as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # set `date` column as index and sort by index\n",
    "# df_merged = df_merged.set_index('date').sort_index()\n",
    "\n",
    "# # take a look\n",
    "# df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106503843, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                object\n",
       "subjects             object\n",
       "date                 object\n",
       "format_group       category\n",
       "format_subgroup    category\n",
       "category_group     category\n",
       "age_group          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I may be able to drop even more columns (thinking especially of `title` and `subjects`), since I'll mostly be looking at sheer numbers of items checked out each day. I'll keep them in for now in case they end up being useful for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 Save\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "Due to some several kernel and zsh shell crashes, I'm going to try to save the DataFrame in batches of 10 million rows.\n",
    "\n",
    "*NOTE: Save time for 10 million rows takes about 5 minutes and the file size is 290MB. Increasing to 20 million rows seemed to increase save time considerably, and so was interrupted before completing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # loop through index and multiples of 10 million\n",
    "# for ind, i in enumerate(range(0, 110000000, 10000000), 1):\n",
    "    \n",
    "#     # save (via compressed pickle) a dataframe of 10 million rows, use index for unique file names\n",
    "#     df_merged.iloc[i:i+10000000].to_pickle(f'data/seattle_lib_{ind}.pkl', compression='gzip')\n",
    "    \n",
    "#     # print status/time\n",
    "#     status_update(f'File {ind} out of 11 saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous loop appeared to be stuck on the 5th part, so I interrupted the kernel and am attempting to save parts 6-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time = 00:53:04\n",
      "-----------------------\n",
      "File 5 out of 11 saved successfully\n",
      "\n",
      "Current time = 01:18:18\n",
      "-----------------------\n",
      "File 6 out of 11 saved successfully\n",
      "\n",
      "Current time = 02:35:13\n",
      "-----------------------\n",
      "File 7 out of 11 saved successfully\n",
      "\n",
      "Current time = 04:42:51\n",
      "-----------------------\n",
      "File 8 out of 11 saved successfully\n",
      "\n",
      "Current time = 06:06:44\n",
      "-----------------------\n",
      "File 9 out of 11 saved successfully\n",
      "\n",
      "Current time = 06:43:35\n",
      "-----------------------\n",
      "File 10 out of 11 saved successfully\n",
      "\n",
      "CPU times: user 28min 7s, sys: 2h 17min 27s, total: 2h 45min 35s\n",
      "Wall time: 6h 10min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loop through index and multiples of 10 million\n",
    "for ind, i in enumerate(range(50000000, 110000000, 10000000), 5):\n",
    "    \n",
    "    # save (via compressed pickle) a dataframe of 10 million rows, use index for unique file names\n",
    "    df_merged.iloc[i:i+10000000].to_pickle(f'data/seattle_lib_{ind}.pkl', compression='gzip')\n",
    "    \n",
    "    # print status/time\n",
    "    status_update(f'File {ind} out of 11 saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.iloc[40000000:50000000].to_pickle(f'data/seattle_lib_4b.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-b8306b2d38fe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-b8306b2d38fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    please break code\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "please break code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marley me</td>\n",
       "      <td>Comedy films, Married people Drama, Philadelph...</td>\n",
       "      <td>2009-07-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Six feet under The complete fourth season</td>\n",
       "      <td>Video recordings for the hearing impaired, Pro...</td>\n",
       "      <td>2008-10-26</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doctor Who The next doctor</td>\n",
       "      <td>London England Drama, Doctor Who Fictitious ch...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>School ties</td>\n",
       "      <td>Antisemitism Drama, Video recordings for the h...</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                                   Firewall   \n",
       "1                                  Marley me   \n",
       "2  Six feet under The complete fourth season   \n",
       "3                 Doctor Who The next doctor   \n",
       "4                                School ties   \n",
       "\n",
       "                                            subjects        date format_group  \\\n",
       "0  Kidnapping Drama, Video recordings for the hea...  2008-02-13        Media   \n",
       "1  Comedy films, Married people Drama, Philadelph...  2009-07-03        Media   \n",
       "2  Video recordings for the hearing impaired, Pro...  2008-10-26        Media   \n",
       "3  London England Drama, Doctor Who Fictitious ch...  2010-11-10        Media   \n",
       "4  Antisemitism Drama, Video recordings for the h...  2008-12-28        Media   \n",
       "\n",
       "  format_subgroup category_group age_group  \n",
       "0      Video Disc        Fiction     Adult  \n",
       "1      Video Disc        Fiction     Adult  \n",
       "2      Video Disc        Fiction     Adult  \n",
       "3      Video Disc        Fiction     Adult  \n",
       "4      Video Disc        Fiction     Adult  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_merged.iloc[:10000000]\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for duplicates and assumption of data integrity\n",
    "\n",
    "The issue of checking for duplicates with this dataset is that duplicates are acceptable! It is very likely that the same item is checked out from either the same or different branches on a single day. Multiple copies of a book, for example, can be stored in one branch or across several branches.\n",
    "\n",
    "NOTE: More investigation on the uniqueness of an item's call number could potentially solve this and allow me to check for actual duplicate rows. For the time being, I will assume the data is almost entirely, if not entirely, accurate.\n",
    "\n",
    "Checking for duplicates can be done using the code below, although it may take quite awhile to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.16 s, sys: 673 ms, total: 9.83 s\n",
      "Wall time: 9.84 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pirates of the Caribbean At worlds end</td>\n",
       "      <td>Adventure films, Fantasy films, Comedy films, ...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Ray</td>\n",
       "      <td>Video recordings for the hearing impaired, Fea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>No reservations</td>\n",
       "      <td>Comedy films, Man woman relationships Drama, V...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519</th>\n",
       "      <td>Charlie Wilsons war</td>\n",
       "      <td>Legislators United States Drama, Video recordi...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27006</th>\n",
       "      <td>Georgia rule</td>\n",
       "      <td>Comedy films, Mothers and daughters Drama, Vid...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42380</th>\n",
       "      <td>Bury my heart at Wounded Knee</td>\n",
       "      <td>Video recordings for the hearing impaired, Uni...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42860</th>\n",
       "      <td>Nacho Libre</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45389</th>\n",
       "      <td>Closer</td>\n",
       "      <td>Man woman relationships England London Drama, ...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57255</th>\n",
       "      <td>Georgia rule</td>\n",
       "      <td>Comedy films, Mothers and daughters Drama, Vid...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66732</th>\n",
       "      <td>Margot at the wedding</td>\n",
       "      <td>Comedy films, Mothers and sons Drama, Problem ...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66931</th>\n",
       "      <td>Eastern promises</td>\n",
       "      <td>London England Drama, Video recordings for the...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69553</th>\n",
       "      <td>unfinished life</td>\n",
       "      <td>Video recordings for the hearing impaired, Fea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82918</th>\n",
       "      <td>In the valley of Elah</td>\n",
       "      <td>Mystery films, Detectives Drama, Fathers and s...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84359</th>\n",
       "      <td>Eastern promises</td>\n",
       "      <td>London England Drama, Video recordings for the...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91443</th>\n",
       "      <td>Into the wild</td>\n",
       "      <td>Adventure films, Video recordings for the hear...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93017</th>\n",
       "      <td>Gone baby gone</td>\n",
       "      <td>Private investigators Drama, Kidnapping Drama,...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95624</th>\n",
       "      <td>Pink Panther</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100300</th>\n",
       "      <td>Waitress</td>\n",
       "      <td>Comedy films, Married people Drama, Pregnant w...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102888</th>\n",
       "      <td>Blades of glory</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117496</th>\n",
       "      <td>Coach Carter</td>\n",
       "      <td>Video recordings for the hearing impaired, Fea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126033</th>\n",
       "      <td>Coach Carter</td>\n",
       "      <td>Video recordings for the hearing impaired, Fea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143509</th>\n",
       "      <td>Looking for comedy in the Muslim world</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146811</th>\n",
       "      <td>Halloween</td>\n",
       "      <td>Horror films, Serial murderers Drama, Video re...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149247</th>\n",
       "      <td>Gone baby gone</td>\n",
       "      <td>Private investigators Drama, Kidnapping Drama,...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151953</th>\n",
       "      <td>Junior Bonner</td>\n",
       "      <td>Rodeos Drama, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163461</th>\n",
       "      <td>Proof</td>\n",
       "      <td>Fathers and daughters Drama, Mental illness Dr...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167333</th>\n",
       "      <td>Bee movie</td>\n",
       "      <td>Comedy films, New York N Y Drama, Human animal...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168104</th>\n",
       "      <td>Proof</td>\n",
       "      <td>Fathers and daughters Drama, Mental illness Dr...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173943</th>\n",
       "      <td>unfinished life</td>\n",
       "      <td>Video recordings for the hearing impaired, Fea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178168</th>\n",
       "      <td>Bridget Jones the edge of reason</td>\n",
       "      <td>Man woman relationships Drama, Video recording...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190633</th>\n",
       "      <td>devil wears Prada</td>\n",
       "      <td>Video recordings for the hearing impaired, Com...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193404</th>\n",
       "      <td>Bee movie</td>\n",
       "      <td>Comedy films, New York N Y Drama, Human animal...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201586</th>\n",
       "      <td>Margot at the wedding</td>\n",
       "      <td>Comedy films, Mothers and sons Drama, Problem ...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202314</th>\n",
       "      <td>Waitress</td>\n",
       "      <td>Comedy films, Married people Drama, Pregnant w...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207505</th>\n",
       "      <td>Bridget Jones the edge of reason</td>\n",
       "      <td>Man woman relationships Drama, Video recording...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208416</th>\n",
       "      <td>Jindabyne</td>\n",
       "      <td>Murder Drama, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213925</th>\n",
       "      <td>Waitress</td>\n",
       "      <td>Comedy films, Married people Drama, Pregnant w...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217478</th>\n",
       "      <td>devil wears Prada</td>\n",
       "      <td>Video recordings for the hearing impaired, Com...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235591</th>\n",
       "      <td>Jindabyne</td>\n",
       "      <td>Murder Drama, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235639</th>\n",
       "      <td>No reservations</td>\n",
       "      <td>Comedy films, Man woman relationships Drama, V...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243951</th>\n",
       "      <td>Closer</td>\n",
       "      <td>Man woman relationships England London Drama, ...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244533</th>\n",
       "      <td>Bridget Jones the edge of reason</td>\n",
       "      <td>Man woman relationships Drama, Video recording...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250438</th>\n",
       "      <td>Into the wild</td>\n",
       "      <td>Adventure films, Video recordings for the hear...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252178</th>\n",
       "      <td>Nacho Libre</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258153</th>\n",
       "      <td>Pink Panther</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265741</th>\n",
       "      <td>Lost in translation</td>\n",
       "      <td>Friendship Drama, Video recordings for the hea...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270774</th>\n",
       "      <td>Charlie Wilsons war</td>\n",
       "      <td>Legislators United States Drama, Video recordi...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281276</th>\n",
       "      <td>Looking for comedy in the Muslim world</td>\n",
       "      <td>Comedy films, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283222</th>\n",
       "      <td>Junior Bonner</td>\n",
       "      <td>Rodeos Drama, Video recordings for the hearing...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292565</th>\n",
       "      <td>To kill a mockingbird</td>\n",
       "      <td>African Americans Crimes against Drama, Trials...</td>\n",
       "      <td>2008-06-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "5       Pirates of the Caribbean At worlds end   \n",
       "662                                        Ray   \n",
       "14387                          No reservations   \n",
       "19519                      Charlie Wilsons war   \n",
       "27006                             Georgia rule   \n",
       "42380            Bury my heart at Wounded Knee   \n",
       "42860                              Nacho Libre   \n",
       "45389                                   Closer   \n",
       "57255                             Georgia rule   \n",
       "66732                    Margot at the wedding   \n",
       "66931                         Eastern promises   \n",
       "69553                          unfinished life   \n",
       "82918                    In the valley of Elah   \n",
       "84359                         Eastern promises   \n",
       "91443                            Into the wild   \n",
       "93017                           Gone baby gone   \n",
       "95624                             Pink Panther   \n",
       "100300                                Waitress   \n",
       "102888                         Blades of glory   \n",
       "117496                            Coach Carter   \n",
       "126033                            Coach Carter   \n",
       "143509  Looking for comedy in the Muslim world   \n",
       "146811                               Halloween   \n",
       "149247                          Gone baby gone   \n",
       "151953                           Junior Bonner   \n",
       "163461                                   Proof   \n",
       "167333                               Bee movie   \n",
       "168104                                   Proof   \n",
       "173943                         unfinished life   \n",
       "178168        Bridget Jones the edge of reason   \n",
       "190633                       devil wears Prada   \n",
       "193404                               Bee movie   \n",
       "201586                   Margot at the wedding   \n",
       "202314                                Waitress   \n",
       "207505        Bridget Jones the edge of reason   \n",
       "208416                               Jindabyne   \n",
       "213925                                Waitress   \n",
       "217478                       devil wears Prada   \n",
       "235591                               Jindabyne   \n",
       "235639                         No reservations   \n",
       "243951                                  Closer   \n",
       "244533        Bridget Jones the edge of reason   \n",
       "250438                           Into the wild   \n",
       "252178                             Nacho Libre   \n",
       "258153                            Pink Panther   \n",
       "265741                     Lost in translation   \n",
       "270774                     Charlie Wilsons war   \n",
       "281276  Looking for comedy in the Muslim world   \n",
       "283222                           Junior Bonner   \n",
       "292565                   To kill a mockingbird   \n",
       "\n",
       "                                                 subjects        date  \\\n",
       "5       Adventure films, Fantasy films, Comedy films, ...  2008-06-03   \n",
       "662     Video recordings for the hearing impaired, Fea...  2008-06-03   \n",
       "14387   Comedy films, Man woman relationships Drama, V...  2008-06-03   \n",
       "19519   Legislators United States Drama, Video recordi...  2008-06-03   \n",
       "27006   Comedy films, Mothers and daughters Drama, Vid...  2008-06-03   \n",
       "42380   Video recordings for the hearing impaired, Uni...  2008-06-03   \n",
       "42860   Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "45389   Man woman relationships England London Drama, ...  2008-06-03   \n",
       "57255   Comedy films, Mothers and daughters Drama, Vid...  2008-06-03   \n",
       "66732   Comedy films, Mothers and sons Drama, Problem ...  2008-06-03   \n",
       "66931   London England Drama, Video recordings for the...  2008-06-03   \n",
       "69553   Video recordings for the hearing impaired, Fea...  2008-06-03   \n",
       "82918   Mystery films, Detectives Drama, Fathers and s...  2008-06-03   \n",
       "84359   London England Drama, Video recordings for the...  2008-06-03   \n",
       "91443   Adventure films, Video recordings for the hear...  2008-06-03   \n",
       "93017   Private investigators Drama, Kidnapping Drama,...  2008-06-03   \n",
       "95624   Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "100300  Comedy films, Married people Drama, Pregnant w...  2008-06-03   \n",
       "102888  Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "117496  Video recordings for the hearing impaired, Fea...  2008-06-03   \n",
       "126033  Video recordings for the hearing impaired, Fea...  2008-06-03   \n",
       "143509  Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "146811  Horror films, Serial murderers Drama, Video re...  2008-06-03   \n",
       "149247  Private investigators Drama, Kidnapping Drama,...  2008-06-03   \n",
       "151953  Rodeos Drama, Video recordings for the hearing...  2008-06-03   \n",
       "163461  Fathers and daughters Drama, Mental illness Dr...  2008-06-03   \n",
       "167333  Comedy films, New York N Y Drama, Human animal...  2008-06-03   \n",
       "168104  Fathers and daughters Drama, Mental illness Dr...  2008-06-03   \n",
       "173943  Video recordings for the hearing impaired, Fea...  2008-06-03   \n",
       "178168  Man woman relationships Drama, Video recording...  2008-06-03   \n",
       "190633  Video recordings for the hearing impaired, Com...  2008-06-03   \n",
       "193404  Comedy films, New York N Y Drama, Human animal...  2008-06-03   \n",
       "201586  Comedy films, Mothers and sons Drama, Problem ...  2008-06-03   \n",
       "202314  Comedy films, Married people Drama, Pregnant w...  2008-06-03   \n",
       "207505  Man woman relationships Drama, Video recording...  2008-06-03   \n",
       "208416  Murder Drama, Video recordings for the hearing...  2008-06-03   \n",
       "213925  Comedy films, Married people Drama, Pregnant w...  2008-06-03   \n",
       "217478  Video recordings for the hearing impaired, Com...  2008-06-03   \n",
       "235591  Murder Drama, Video recordings for the hearing...  2008-06-03   \n",
       "235639  Comedy films, Man woman relationships Drama, V...  2008-06-03   \n",
       "243951  Man woman relationships England London Drama, ...  2008-06-03   \n",
       "244533  Man woman relationships Drama, Video recording...  2008-06-03   \n",
       "250438  Adventure films, Video recordings for the hear...  2008-06-03   \n",
       "252178  Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "258153  Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "265741  Friendship Drama, Video recordings for the hea...  2008-06-03   \n",
       "270774  Legislators United States Drama, Video recordi...  2008-06-03   \n",
       "281276  Comedy films, Video recordings for the hearing...  2008-06-03   \n",
       "283222  Rodeos Drama, Video recordings for the hearing...  2008-06-03   \n",
       "292565  African Americans Crimes against Drama, Trials...  2008-06-03   \n",
       "\n",
       "       format_group format_subgroup category_group age_group  \n",
       "5             Media      Video Disc        Fiction     Adult  \n",
       "662           Media      Video Disc        Fiction     Adult  \n",
       "14387         Media      Video Disc        Fiction     Adult  \n",
       "19519         Media      Video Disc        Fiction     Adult  \n",
       "27006         Media      Video Disc        Fiction     Adult  \n",
       "42380         Media      Video Disc        Fiction     Adult  \n",
       "42860         Media      Video Disc        Fiction     Adult  \n",
       "45389         Media      Video Disc        Fiction     Adult  \n",
       "57255         Media      Video Disc        Fiction     Adult  \n",
       "66732         Media      Video Disc        Fiction     Adult  \n",
       "66931         Media      Video Disc        Fiction     Adult  \n",
       "69553         Media      Video Disc        Fiction     Adult  \n",
       "82918         Media      Video Disc        Fiction     Adult  \n",
       "84359         Media      Video Disc        Fiction     Adult  \n",
       "91443         Media      Video Disc        Fiction     Adult  \n",
       "93017         Media      Video Disc        Fiction     Adult  \n",
       "95624         Media      Video Disc        Fiction     Adult  \n",
       "100300        Media      Video Disc        Fiction     Adult  \n",
       "102888        Media      Video Disc        Fiction     Adult  \n",
       "117496        Media      Video Disc        Fiction     Adult  \n",
       "126033        Media      Video Disc        Fiction     Adult  \n",
       "143509        Media      Video Disc        Fiction     Adult  \n",
       "146811        Media      Video Disc        Fiction     Adult  \n",
       "149247        Media      Video Disc        Fiction     Adult  \n",
       "151953        Media      Video Disc        Fiction     Adult  \n",
       "163461        Media      Video Disc        Fiction     Adult  \n",
       "167333        Media      Video Disc        Fiction     Adult  \n",
       "168104        Media      Video Disc        Fiction     Adult  \n",
       "173943        Media      Video Disc        Fiction     Adult  \n",
       "178168        Media      Video Disc        Fiction     Adult  \n",
       "190633        Media      Video Disc        Fiction     Adult  \n",
       "193404        Media      Video Disc        Fiction     Adult  \n",
       "201586        Media      Video Disc        Fiction     Adult  \n",
       "202314        Media      Video Disc        Fiction     Adult  \n",
       "207505        Media      Video Disc        Fiction     Adult  \n",
       "208416        Media      Video Disc        Fiction     Adult  \n",
       "213925        Media      Video Disc        Fiction     Adult  \n",
       "217478        Media      Video Disc        Fiction     Adult  \n",
       "235591        Media      Video Disc        Fiction     Adult  \n",
       "235639        Media      Video Disc        Fiction     Adult  \n",
       "243951        Media      Video Disc        Fiction     Adult  \n",
       "244533        Media      Video Disc        Fiction     Adult  \n",
       "250438        Media      Video Disc        Fiction     Adult  \n",
       "252178        Media      Video Disc        Fiction     Adult  \n",
       "258153        Media      Video Disc        Fiction     Adult  \n",
       "265741        Media      Video Disc        Fiction     Adult  \n",
       "270774        Media      Video Disc        Fiction     Adult  \n",
       "281276        Media      Video Disc        Fiction     Adult  \n",
       "283222        Media      Video Disc        Fiction     Adult  \n",
       "292565        Media      Video Disc        Fiction     Adult  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# df_merged[df_merged.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 15s, sys: 1min 56s, total: 5min 12s\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date        format_group  format_subgroup  category_group     age_group\n",
       "2008-01-02  Electronic    Art              Fiction            Adult        0\n",
       "                                                              Juvenile     0\n",
       "                                                              Teen         0\n",
       "                                           Interlibrary Loan  Adult        0\n",
       "                                                              Juvenile     0\n",
       "                                                                          ..\n",
       "2019-07-18  Print         Video Tape       Temporary          Juvenile     0\n",
       "                                                              Teen         0\n",
       "                                           WTBBL              Adult        0\n",
       "                                                              Juvenile     0\n",
       "                                                              Teen         0\n",
       "Length: 9139500, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['date', 'format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "test[cols].groupby(cols).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Print         59685137\n",
       "Media         46618209\n",
       "Other           200478\n",
       "Equipment           18\n",
       "Electronic           1\n",
       "Name: format_group, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.format_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Book              59486648\n",
       "Video Disc        30287406\n",
       "Audio Disc        11238813\n",
       "Audiobook Disc     2695078\n",
       "Video Tape         1474457\n",
       "Kit                 626008\n",
       "Audiobook Tape      240328\n",
       "Music Score         130486\n",
       "Audio Tape           45946\n",
       "Folder               23900\n",
       "Data Disc             9886\n",
       "Periodical             623\n",
       "Document               471\n",
       "Art                    129\n",
       "Film                    81\n",
       "Name: format_subgroup, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.format_subgroup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Fiction, Nonfiction, Language, Miscellaneous, Interlibrary Loan, Reference, On Order, NaN, Temporary, WTBBL]\n",
       "Categories (9, object): [Fiction, Nonfiction, Language, Miscellaneous, ..., Reference, On Order, Temporary, WTBBL]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.category_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fiction              65292861\n",
       "Nonfiction           37539002\n",
       "Miscellaneous         1734282\n",
       "Language              1679452\n",
       "Interlibrary Loan      192959\n",
       "Reference               57420\n",
       "On Order                 7422\n",
       "Temporary                  40\n",
       "WTBBL                      26\n",
       "Periodical                  0\n",
       "Name: category_group, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.category_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these (`Miscellaneous`, `On Order`, `Temporary`) can be simplified into an `Other` category, since their current category doesn't provide any valuable information in terms of what the actual item is. I was originally considering including `Interlibrary Loan` in the `Other` category as well, but it may be interesting to see the number of activity between branches, so I'll leave it in for now.\n",
    "\n",
    "Based on some research, `WTBBL` stands for \"Washington Talking Book Library\" and includes materials for folks with visual impairments. I was interested in looking into this, but since the numbers are so low, I think I'll also group that into the `Other` category, as I assume this `category_group` value deal with *equipment* that can be rented.\n",
    "\n",
    "I will also convert `Periodical` to other; even though the count for it is 0, I assume this may relate to to items that have a `format_subgroup` value but no `category_group` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_values = ['Miscellaneous', 'On Order', 'Temporary', 'WTBBL', 'Periodical']\n",
    "\n",
    "test['category_group'] = np.where(test.category_group in convert_values, 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fiction'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[0,'category_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.3 s, sys: 2min 39s, total: 3min 34s\n",
      "Wall time: 8min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date        format_group\n",
       "2005-04-13  Print           10041\n",
       "            Media            6397\n",
       "            Other              32\n",
       "            Equipment           1\n",
       "2005-04-14  Print            6267\n",
       "                            ...  \n",
       "2020-12-12  Other               7\n",
       "2020-12-13  Print            3225\n",
       "            Media             760\n",
       "2020-12-14  Print            1342\n",
       "            Media             282\n",
       "Name: format_group, Length: 16107, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_merged.groupby('date').format_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 3.79 s, total: 43.6 s\n",
      "Wall time: 43.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>format_group</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Equipment</th>\n",
       "      <th>Media</th>\n",
       "      <th>Other</th>\n",
       "      <th>Print</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-04-13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6397.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4015.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5351.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>7494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2169.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "format_group  Electronic  Equipment   Media  Other    Print\n",
       "date                                                       \n",
       "2005-04-13           0.0        1.0  6397.0   32.0  10041.0\n",
       "2005-04-14           0.0        1.0  4015.0   75.0   6267.0\n",
       "2005-04-15           0.0        0.0  5351.0   51.0   7494.0\n",
       "2005-04-16           0.0        0.0   552.0    0.0    806.0\n",
       "2005-04-17           0.0        0.0  1555.0    8.0   2992.0\n",
       "...                  ...        ...     ...    ...      ...\n",
       "2020-12-10           0.0        0.0  1800.0    2.0   3913.0\n",
       "2020-12-11           0.0        0.0  1417.0    3.0   4234.0\n",
       "2020-12-12           0.0        0.0  2169.0    7.0   4681.0\n",
       "2020-12-13           0.0        0.0   760.0    0.0   3225.0\n",
       "2020-12-14           0.0        0.0   282.0    0.0   1342.0\n",
       "\n",
       "[5470 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_merged.groupby('date').format_group.value_counts().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 1.19 s, total: 20.5 s\n",
      "Wall time: 20.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date\n",
       "2005-04-13    16471\n",
       "2005-04-14    10358\n",
       "2005-04-15    12896\n",
       "2005-04-16     1358\n",
       "2005-04-17     4555\n",
       "              ...  \n",
       "2020-12-10     5715\n",
       "2020-12-11     5654\n",
       "2020-12-12     6857\n",
       "2020-12-13     3985\n",
       "2020-12-14     1624\n",
       "Length: 5470, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_merged.groupby('date').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['date', 'format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "df_merged.groupby(cols).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 2.84 s, total: 28.8 s\n",
      "Wall time: 29.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-04-13</th>\n",
       "      <td>16471</td>\n",
       "      <td>16415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-14</th>\n",
       "      <td>10358</td>\n",
       "      <td>10276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>12896</td>\n",
       "      <td>12842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-16</th>\n",
       "      <td>1358</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-17</th>\n",
       "      <td>4555</td>\n",
       "      <td>4535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-10</th>\n",
       "      <td>5715</td>\n",
       "      <td>5708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-11</th>\n",
       "      <td>5654</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-12</th>\n",
       "      <td>6857</td>\n",
       "      <td>6846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>3985</td>\n",
       "      <td>3978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-14</th>\n",
       "      <td>1624</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            format_group  format_subgroup\n",
       "date                                     \n",
       "2005-04-13         16471            16415\n",
       "2005-04-14         10358            10276\n",
       "2005-04-15         12896            12842\n",
       "2005-04-16          1358             1357\n",
       "2005-04-17          4555             4535\n",
       "...                  ...              ...\n",
       "2020-12-10          5715             5708\n",
       "2020-12-11          5654             5648\n",
       "2020-12-12          6857             6846\n",
       "2020-12-13          3985             3978\n",
       "2020-12-14          1624             1624\n",
       "\n",
       "[5470 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['format_group', 'format_subgroup',\n",
    "#         'category_group', 'age_group'\n",
    "       ]\n",
    "\n",
    "df_merged.groupby('date')[cols].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marley me</td>\n",
       "      <td>Comedy films, Married people Drama, Philadelph...</td>\n",
       "      <td>2009-07-03</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Six feet under The complete fourth season</td>\n",
       "      <td>Video recordings for the hearing impaired, Pro...</td>\n",
       "      <td>2008-10-26</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doctor Who The next doctor</td>\n",
       "      <td>London England Drama, Doctor Who Fictitious ch...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>School ties</td>\n",
       "      <td>Antisemitism Drama, Video recordings for the h...</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>Unbreakable</td>\n",
       "      <td>Heroes Drama, Supernatural Drama, Survival Dra...</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>DCs legends of tomorrow The complete second se...</td>\n",
       "      <td>Good and evil Drama, Time travel Drama, Superh...</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>Unfriended Dark web</td>\n",
       "      <td>Stalking victims Drama, Murder Drama, Internet...</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>Oh God Book II</td>\n",
       "      <td>Presence of God Drama, Comedy films, Feature f...</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>Americans The complete final season</td>\n",
       "      <td>United States Federal Bureau of Investigation ...</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "0                                                 Firewall   \n",
       "1                                                Marley me   \n",
       "2                Six feet under The complete fourth season   \n",
       "3                               Doctor Who The next doctor   \n",
       "4                                              School ties   \n",
       "...                                                    ...   \n",
       "9999995                                        Unbreakable   \n",
       "9999996  DCs legends of tomorrow The complete second se...   \n",
       "9999997                                Unfriended Dark web   \n",
       "9999998                                     Oh God Book II   \n",
       "9999999                Americans The complete final season   \n",
       "\n",
       "                                                  subjects        date  \\\n",
       "0        Kidnapping Drama, Video recordings for the hea...  2008-02-13   \n",
       "1        Comedy films, Married people Drama, Philadelph...  2009-07-03   \n",
       "2        Video recordings for the hearing impaired, Pro...  2008-10-26   \n",
       "3        London England Drama, Doctor Who Fictitious ch...  2010-11-10   \n",
       "4        Antisemitism Drama, Video recordings for the h...  2008-12-28   \n",
       "...                                                    ...         ...   \n",
       "9999995  Heroes Drama, Supernatural Drama, Survival Dra...  2019-07-18   \n",
       "9999996  Good and evil Drama, Time travel Drama, Superh...  2019-07-18   \n",
       "9999997  Stalking victims Drama, Murder Drama, Internet...  2019-07-18   \n",
       "9999998  Presence of God Drama, Comedy films, Feature f...  2019-07-18   \n",
       "9999999  United States Federal Bureau of Investigation ...  2019-07-18   \n",
       "\n",
       "        format_group format_subgroup category_group age_group  \n",
       "0              Media      Video Disc        Fiction     Adult  \n",
       "1              Media      Video Disc        Fiction     Adult  \n",
       "2              Media      Video Disc        Fiction     Adult  \n",
       "3              Media      Video Disc        Fiction     Adult  \n",
       "4              Media      Video Disc        Fiction     Adult  \n",
       "...              ...             ...            ...       ...  \n",
       "9999995        Media      Video Disc        Fiction     Adult  \n",
       "9999996        Media      Video Disc        Fiction     Adult  \n",
       "9999997        Media      Video Disc        Fiction     Adult  \n",
       "9999998        Media      Video Disc        Fiction     Adult  \n",
       "9999999        Media      Video Disc        Fiction     Adult  \n",
       "\n",
       "[10000000 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.category_group == 'Fiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adult       71587854\n",
       "Juvenile    31006173\n",
       "Teen         3909816\n",
       "Name: age_group, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.age_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['category_group'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 1min 21s, total: 4min 32s\n",
      "Wall time: 4min 33s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th colspan=\"21\" halign=\"left\">2008-01-02</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_group</th>\n",
       "      <th colspan=\"21\" halign=\"left\">Electronic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_subgroup</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Art</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Audio Disc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_group</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Fiction</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Interlibrary Loan</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Language</th>\n",
       "      <th>Miscellaneous</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Miscellaneous</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nonfiction</th>\n",
       "      <th colspan=\"3\" halign=\"left\">On Order</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Periodical</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_group</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>...</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "      <th>Teen</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Juvenile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(2008-01-02, Electronic, Art, Fiction, Adult), (2008-01-02, Electronic, Art, Fiction, Juvenile), (2008-01-02, Electronic, Art, Fiction, Teen), (2008-01-02, Electronic, Art, Interlibrary Loan, Adult), (2008-01-02, Electronic, Art, Interlibrary Loan, Juvenile), (2008-01-02, Electronic, Art, Interlibrary Loan, Teen), (2008-01-02, Electronic, Art, Language, Adult), (2008-01-02, Electronic, Art, Language, Juvenile), (2008-01-02, Electronic, Art, Language, Teen), (2008-01-02, Electronic, Art, Miscellaneous, Adult), (2008-01-02, Electronic, Art, Miscellaneous, Juvenile), (2008-01-02, Electronic, Art, Miscellaneous, Teen), (2008-01-02, Electronic, Art, Nonfiction, Adult), (2008-01-02, Electronic, Art, Nonfiction, Juvenile), (2008-01-02, Electronic, Art, Nonfiction, Teen), (2008-01-02, Electronic, Art, On Order, Adult), (2008-01-02, Electronic, Art, On Order, Juvenile), (2008-01-02, Electronic, Art, On Order, Teen), (2008-01-02, Electronic, Art, Periodical, Adult), (2008-01-02, Electronic, Art, Periodical, Juvenile), (2008-01-02, Electronic, Art, Periodical, Teen), (2008-01-02, Electronic, Art, Reference, Adult), (2008-01-02, Electronic, Art, Reference, Juvenile), (2008-01-02, Electronic, Art, Reference, Teen), (2008-01-02, Electronic, Art, Temporary, Adult), (2008-01-02, Electronic, Art, Temporary, Juvenile), (2008-01-02, Electronic, Art, Temporary, Teen), (2008-01-02, Electronic, Art, WTBBL, Adult), (2008-01-02, Electronic, Art, WTBBL, Juvenile), (2008-01-02, Electronic, Art, WTBBL, Teen), (2008-01-02, Electronic, Audio Disc, Fiction, Adult), (2008-01-02, Electronic, Audio Disc, Fiction, Juvenile), (2008-01-02, Electronic, Audio Disc, Fiction, Teen), (2008-01-02, Electronic, Audio Disc, Interlibrary Loan, Adult), (2008-01-02, Electronic, Audio Disc, Interlibrary Loan, Juvenile), (2008-01-02, Electronic, Audio Disc, Interlibrary Loan, Teen), (2008-01-02, Electronic, Audio Disc, Language, Adult), (2008-01-02, Electronic, Audio Disc, Language, Juvenile), (2008-01-02, Electronic, Audio Disc, Language, Teen), (2008-01-02, Electronic, Audio Disc, Miscellaneous, Adult), (2008-01-02, Electronic, Audio Disc, Miscellaneous, Juvenile), (2008-01-02, Electronic, Audio Disc, Miscellaneous, Teen), (2008-01-02, Electronic, Audio Disc, Nonfiction, Adult), (2008-01-02, Electronic, Audio Disc, Nonfiction, Juvenile), (2008-01-02, Electronic, Audio Disc, Nonfiction, Teen), (2008-01-02, Electronic, Audio Disc, On Order, Adult), (2008-01-02, Electronic, Audio Disc, On Order, Juvenile), (2008-01-02, Electronic, Audio Disc, On Order, Teen), (2008-01-02, Electronic, Audio Disc, Periodical, Adult), (2008-01-02, Electronic, Audio Disc, Periodical, Juvenile)]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 50 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['date', 'format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "test[cols].groupby(cols).count().head(50).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>514</td>\n",
       "      <td>517</td>\n",
       "      <td>517</td>\n",
       "      <td>517</td>\n",
       "      <td>517</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>373</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>398</td>\n",
       "      <td>401</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-05</th>\n",
       "      <td>392</td>\n",
       "      <td>395</td>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-06</th>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>1247</td>\n",
       "      <td>1247</td>\n",
       "      <td>1247</td>\n",
       "      <td>1247</td>\n",
       "      <td>1247</td>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-15</th>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-16</th>\n",
       "      <td>2077</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077</td>\n",
       "      <td>2077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-17</th>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-18</th>\n",
       "      <td>1361</td>\n",
       "      <td>1361</td>\n",
       "      <td>1361</td>\n",
       "      <td>1361</td>\n",
       "      <td>1361</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4062 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            title  subjects  format_group  format_subgroup  category_group  \\\n",
       "date                                                                         \n",
       "2008-01-02    514       517           517              517             517   \n",
       "2008-01-03    373       378           378              378             378   \n",
       "2008-01-04    398       401           404              404             404   \n",
       "2008-01-05    392       395           396              396             396   \n",
       "2008-01-06    170       172           173              173             173   \n",
       "...           ...       ...           ...              ...             ...   \n",
       "2019-07-14   1247      1247          1247             1247            1247   \n",
       "2019-07-15   2364      2364          2364             2364            2364   \n",
       "2019-07-16   2077      2077          2077             2077            2077   \n",
       "2019-07-17   1947      1947          1947             1947            1947   \n",
       "2019-07-18   1361      1361          1361             1361            1361   \n",
       "\n",
       "            age_group  \n",
       "date                   \n",
       "2008-01-02        517  \n",
       "2008-01-03        378  \n",
       "2008-01-04        404  \n",
       "2008-01-05        396  \n",
       "2008-01-06        173  \n",
       "...               ...  \n",
       "2019-07-14       1247  \n",
       "2019-07-15       2364  \n",
       "2019-07-16       2077  \n",
       "2019-07-17       1947  \n",
       "2019-07-18       1361  \n",
       "\n",
       "[4062 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test.groupby('date').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, subjects, date, format_group, format_subgroup, category_group, age_group]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.date == '2008-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['date', 'format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "test[cols].value_counts(subset=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAVEYARD\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # list of columns to keep\n",
    "# keep_cols = ['title', 'subjects', 'date', 'format_group', 'format_subgroup',\n",
    "#              'category_group', 'age_group']\n",
    "\n",
    "# # drop columns\n",
    "# df_merged = df_merged[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code_type == 'ItemType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'nadvd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'acdvd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.code_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_item = dd[dd.code_type == 'ItemType'][['code', 'description', 'format_group', 'format_subgroup', 'category_group', \n",
    "             'category_subgroup', 'age_group']]\n",
    "\n",
    "dd_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_item2 = dd[dd.code_type == 'ItemCollection'][['code', 'description', 'format_group', 'format_subgroup', 'category_group', \n",
    "             'category_subgroup', 'age_group']]\n",
    "\n",
    "dd_item2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.item_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_loc = dd[dd.code_type == 'Location'][['code', 'description']]\n",
    "\n",
    "dd_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.merge(dd_item, left_on='item_type', right_on='code')\n",
    "# test = test.merge(dd_loc, left_on='collection', right_on='code')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.format_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.collection.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = df.merge(dd_item2, left_on='Collection', right_on='code')\n",
    "\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('format_group').category_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.groupby('format_group').category_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'nybot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.collection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_loc.code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cod for cod in df.collection.unique() if cod in dd_loc.code.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Using the code below on 10 million rows is almost 39% faster and results in a saved file that is *nearly the same size as the original CSV file* (file is 23.22GB!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_merged.to_hdf('data/seattle_lib_temp_ten_mil__alt.hdf', 'mydata', format='table', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.title=='reader'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
