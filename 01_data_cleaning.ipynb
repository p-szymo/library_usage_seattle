{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Usage in Seattle, 2005-2020\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The data is the [Checkouts by Title (Physical Items)](https://data.seattle.gov/Community/Checkouts-By-Title-Physical-Items-/5src-czff) dataset from [Seattle Open Data](https://data.seattle.gov/) and was downloaded on December 15, 2020.\n",
    "\n",
    "This notebook is designed to load a downloaded CSV file, merge it with item-specific information, convert it to a time-series-ready DataFrame, and save that as a compressed pickle file.\n",
    "\n",
    "*Note: This dataset is updated weekly; the more data, the longer the load times will be.*\n",
    "\n",
    "In the future, it may be a good idea to look into adding [API](https://dev.socrata.com/foundry/data.seattle.gov/5src-czff) calls into the pipeline, so as to quickly and easily add on the additional weekly data.\n",
    "\n",
    "*Note: Any cell that uses the built-in magic command* `%%time` *takes a significant (or at least not insignificant) time to run.*\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dataframe packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# saving packages\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from functions.data_cleaning import status_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkout data\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "Since the data set is so large, I'll specify only the columns that I want in the DataFrame. This will effectively drop the following columns:\n",
    "- `ID`\n",
    "- `CheckoutYear`\n",
    "- `CallNumber`\n",
    "- `BibNumber`\n",
    "- `ItemBarcode`\n",
    "- `ItemType`\n",
    "    \n",
    "I want to note that the `ItemType` and `Collection` columns are very similar, but the code in the `Collection` column contains more information within the `category_group` column that I add onto the DataFrame using the `data_dictionary.csv` file ([see below](#Load-other-info-from-data-dictionary-and-merge-onto-checkouts-dataset)). More specifically, the `ItemType` code yields mostly \"Miscellaneous\" results, whereas the `Collection` code yields differentiates between \"Fiction\" and \"Nonfiction\", among others. This could be useful information later on, so I found it best to drop the `ItemType` column.\n",
    "\n",
    "Note: In the future, I may consider using this [dataset](https://data.seattle.gov/Community/Library-Collection-Inventory/6vkj-f5xf) to add on branch information (i.e. which branch an item was checked out from), although this data is rather limited (due to privacy concerns) and incomplete (only appears to be collected beginning in 2017). In order to do that I would need to use the `BibNumber` column.\n",
    "\n",
    "#### ⏰ Cell below takes ~14.5 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 20s, sys: 4min 39s, total: 9min\n",
      "Wall time: 14min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# columns to load\n",
    "usecols = ['Collection', 'ItemTitle', 'Subjects', 'CheckoutDateTime']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('data/Checkouts_By_Title__Physical_Items_.csv',\n",
    "#                  nrows=10000000,\n",
    "                 usecols=usecols)\n",
    "\n",
    "# rename columns to my preferred format\n",
    "df.columns = ['collection', 'title', 'subjects', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: While it only took about 25 seconds to load 10 million rows, it takes about 25 minutes to load 106.5 million rows with the same number (5) of columns. In my latest update, I brought it down to 4 columns, which decreased load time to 15.5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106534901, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>02/13/2008 07:38:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nanf</td>\n",
       "      <td>best baby shower book a complete guide for par...</td>\n",
       "      <td>Showers Parties</td>\n",
       "      <td>07/23/2008 02:53:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nyfic</td>\n",
       "      <td>Uglies</td>\n",
       "      <td>Fantasy, Teenage girls Fiction, Beauty Persona...</td>\n",
       "      <td>12/23/2009 04:20:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>napar</td>\n",
       "      <td>doula guide to birth secrets every pregnant wo...</td>\n",
       "      <td>Doulas, Childbirth</td>\n",
       "      <td>11/16/2010 12:04:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canf</td>\n",
       "      <td>Salmon a cookbook</td>\n",
       "      <td>Cookery Salmon</td>\n",
       "      <td>04/26/2009 01:29:00 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  collection                                              title  \\\n",
       "0      nadvd                                           Firewall   \n",
       "1       nanf  best baby shower book a complete guide for par...   \n",
       "2      nyfic                                             Uglies   \n",
       "3      napar  doula guide to birth secrets every pregnant wo...   \n",
       "4       canf                                  Salmon a cookbook   \n",
       "\n",
       "                                            subjects                    date  \n",
       "0  Kidnapping Drama, Video recordings for the hea...  02/13/2008 07:38:00 PM  \n",
       "1                                    Showers Parties  07/23/2008 02:53:00 PM  \n",
       "2  Fantasy, Teenage girls Fiction, Beauty Persona...  12/23/2009 04:20:00 PM  \n",
       "3                                 Doulas, Childbirth  11/16/2010 12:04:00 PM  \n",
       "4                                     Cookery Salmon  04/26/2009 01:29:00 PM  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~3 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 1min 9s, total: 1min 43s\n",
      "Wall time: 3min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "collection          0\n",
       "title          900912\n",
       "subjects      1649522\n",
       "date                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check for nan values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: Even checking for NaN values takes a significant amount of time with this many rows.*\n",
    "\n",
    "The most important columns (`collection` and `date`) have no NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collection    object\n",
       "title         object\n",
       "subjects      object\n",
       "date          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `date` column to datetime\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/13/2008 07:38:00 PM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at an example before conversion\n",
    "df.loc[0, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the format\n",
    "dt_format = '%m/%d/%Y %I:%M:%S %p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~7 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 27s, sys: 15.6 s, total: 6min 43s\n",
      "Wall time: 6min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.date(2008, 2, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert to datetime, dropping the hour-minute-second stamp using the `dt.date` attribute\n",
    "df['date'] = pd.to_datetime(df.date, format=dt_format).dt.date\n",
    "\n",
    "# confirm it worked\n",
    "df.loc[0, 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load other info from data dictionary and merge onto checkouts dataset\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>description</th>\n",
       "      <th>code_type</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>category_subgroup</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cazover</td>\n",
       "      <td>CA7-zine collection oversize</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caziner</td>\n",
       "      <td>CA7-zine collection reference</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cazval</td>\n",
       "      <td>CA7-zine collection valuable mat.</td>\n",
       "      <td>ItemCollection</td>\n",
       "      <td>Print</td>\n",
       "      <td>Book</td>\n",
       "      <td>Periodical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nga</td>\n",
       "      <td>Northgate Branch</td>\n",
       "      <td>Location</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hip</td>\n",
       "      <td>High Point Branch</td>\n",
       "      <td>Location</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      code                        description       code_type format_group  \\\n",
       "0  cazover       CA7-zine collection oversize  ItemCollection        Print   \n",
       "1  caziner      CA7-zine collection reference  ItemCollection        Print   \n",
       "2   cazval  CA7-zine collection valuable mat.  ItemCollection        Print   \n",
       "3      nga                   Northgate Branch        Location          NaN   \n",
       "4      hip                  High Point Branch        Location          NaN   \n",
       "\n",
       "  format_subgroup category_group category_subgroup age_group  \n",
       "0            Book     Periodical               NaN     Adult  \n",
       "1            Book     Periodical               NaN     Adult  \n",
       "2            Book     Periodical               NaN     Adult  \n",
       "3             NaN            NaN               NaN       NaN  \n",
       "4             NaN            NaN               NaN       NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "dd = pd.read_csv('data/data_dictionary.csv')\n",
    "\n",
    "# rename columns to my preferred format\n",
    "dd.columns = ['code', 'description', 'code_type', 'format_group', 'format_subgroup', \n",
    "              'category_group', 'category_subgroup', 'age_group']\n",
    "\n",
    "# take a look\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                 object\n",
       "description          object\n",
       "code_type            object\n",
       "format_group         object\n",
       "format_subgroup      object\n",
       "category_group       object\n",
       "category_subgroup    object\n",
       "age_group            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "dd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will only be using information from codes whose type is \"ItemCollection\", I'll subset the data dictionary down to just those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to only collection codes\n",
    "dd = dd[dd.code_type == 'ItemCollection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                   0\n",
       "description            0\n",
       "code_type              0\n",
       "format_group           0\n",
       "format_subgroup       28\n",
       "category_group         2\n",
       "category_subgroup    391\n",
       "age_group              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nan values\n",
    "dd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, with the size of the eventual DataFrame in mind, I want to drop any unnecessary columns before merging, so I'll drop the following columns:\n",
    "- `description`, since that is superfluous information for this project\n",
    "- `code_type`, since that is superfluous information\n",
    "- `category_subgroup`, since that is mostly NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "dd.drop(columns=['description', 'code_type', 'category_subgroup'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to convert\n",
    "to_convert = ['format_group', 'format_subgroup', 'category_group', 'age_group']\n",
    "\n",
    "# convert to category datatype\n",
    "dd[to_convert] = dd[to_convert].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                 object\n",
       "format_group       category\n",
       "format_subgroup    category\n",
       "category_group     category\n",
       "age_group          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm new datatypes\n",
    "dd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⏰ Cell below takes ~4 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1min 50s, total: 2min 51s\n",
      "Wall time: 4min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>format_group</th>\n",
       "      <th>format_subgroup</th>\n",
       "      <th>category_group</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Kidnapping Drama, Video recordings for the hea...</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Marley me</td>\n",
       "      <td>Comedy films, Married people Drama, Philadelph...</td>\n",
       "      <td>2009-07-03</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Six feet under The complete fourth season</td>\n",
       "      <td>Video recordings for the hearing impaired, Pro...</td>\n",
       "      <td>2008-10-26</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>Doctor Who The next doctor</td>\n",
       "      <td>London England Drama, Doctor Who Fictitious ch...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nadvd</td>\n",
       "      <td>School ties</td>\n",
       "      <td>Antisemitism Drama, Video recordings for the h...</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>nadvd</td>\n",
       "      <td>Media</td>\n",
       "      <td>Video Disc</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  collection                                      title  \\\n",
       "0      nadvd                                   Firewall   \n",
       "1      nadvd                                  Marley me   \n",
       "2      nadvd  Six feet under The complete fourth season   \n",
       "3      nadvd                 Doctor Who The next doctor   \n",
       "4      nadvd                                School ties   \n",
       "\n",
       "                                            subjects        date   code  \\\n",
       "0  Kidnapping Drama, Video recordings for the hea...  2008-02-13  nadvd   \n",
       "1  Comedy films, Married people Drama, Philadelph...  2009-07-03  nadvd   \n",
       "2  Video recordings for the hearing impaired, Pro...  2008-10-26  nadvd   \n",
       "3  London England Drama, Doctor Who Fictitious ch...  2010-11-10  nadvd   \n",
       "4  Antisemitism Drama, Video recordings for the h...  2008-12-28  nadvd   \n",
       "\n",
       "  format_group format_subgroup category_group age_group  \n",
       "0        Media      Video Disc        Fiction     Adult  \n",
       "1        Media      Video Disc        Fiction     Adult  \n",
       "2        Media      Video Disc        Fiction     Adult  \n",
       "3        Media      Video Disc        Fiction     Adult  \n",
       "4        Media      Video Disc        Fiction     Adult  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# merge checkouts dataframe with info from data dictionary\n",
    "df_merged = df.merge(dd, left_on='collection', right_on='code')\n",
    "\n",
    "# take a look\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns\n",
    "\n",
    "I can now drop the `collection` and `code` columns, since those are no longer necessary.\n",
    "\n",
    "*NOTE: Using the Pandas method `.drop()` was taking well over an hour, so I'm going to try to subset it below, to see if that works any faster.*\n",
    "\n",
    "#### ⏰ Cell below takes ~38.5 minutes to run. ⏰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 18min 6s, total: 20min 50s\n",
      "Wall time: 44min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# drop columns\n",
    "df_merged.drop(columns=['collection', 'code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set `date` column as index\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "I've commented out the below code because now *this* has begun to consistently crash the kernel or zsh shell.\n",
    "\n",
    "After thinking more about it, I believe I will end up grouping by the date to get raw numbers for each day (not only total checkouts, but total print checkouts and fiction checkouts, etc.), which can be done before setting the `date` column as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # set `date` column as index and sort by index\n",
    "# df_merged = df_merged.set_index('date').sort_index()\n",
    "\n",
    "# # take a look\n",
    "# df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106503843, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                object\n",
       "subjects             object\n",
       "date                 object\n",
       "format_group       category\n",
       "format_subgroup    category\n",
       "category_group     category\n",
       "age_group          category\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatypes\n",
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I may be able to drop even more columns (thinking especially of `title` and `subjects`), since I'll mostly be looking at sheer numbers of items checked out each day. I'll keep them in for now in case they end up being useful for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 Save\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]\n",
    "\n",
    "Due to some several kernel and zsh shell crashes, I'm going to try to save the DataFrame in batches of 10 million rows.\n",
    "\n",
    "*NOTE: Save time for 10 million rows takes about 5 minutes and the file size is 290MB. Increasing to 20 million rows seemed to increase save time considerably, and so was interrupted before completing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # loop through index and multiples of 10 million\n",
    "# for ind, i in enumerate(range(0, 110000000, 10000000), 1):\n",
    "    \n",
    "#     # save (via compressed pickle) a dataframe of 10 million rows, use index for unique file names\n",
    "#     df_merged.iloc[i:i+10000000].to_pickle(f'data/seattle_lib_{ind}.pkl', compression='gzip')\n",
    "    \n",
    "#     # print status/time\n",
    "#     status_update(f'File {ind} out of 11 saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous loop appeared to be stuck on the 5th part, so I interrupted the kernel and am attempting to save parts 6-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time = 00:53:04\n",
      "-----------------------\n",
      "File 5 out of 11 saved successfully\n",
      "\n",
      "Current time = 01:18:18\n",
      "-----------------------\n",
      "File 6 out of 11 saved successfully\n",
      "\n",
      "Current time = 02:35:13\n",
      "-----------------------\n",
      "File 7 out of 11 saved successfully\n",
      "\n",
      "Current time = 04:42:51\n",
      "-----------------------\n",
      "File 8 out of 11 saved successfully\n",
      "\n",
      "Current time = 06:06:44\n",
      "-----------------------\n",
      "File 9 out of 11 saved successfully\n",
      "\n",
      "Current time = 06:43:35\n",
      "-----------------------\n",
      "File 10 out of 11 saved successfully\n",
      "\n",
      "CPU times: user 28min 7s, sys: 2h 17min 27s, total: 2h 45min 35s\n",
      "Wall time: 6h 10min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loop through index and multiples of 10 million\n",
    "for ind, i in enumerate(range(50000000, 110000000, 10000000), 5):\n",
    "    \n",
    "    # save (via compressed pickle) a dataframe of 10 million rows, use index for unique file names\n",
    "    df_merged.iloc[i:i+10000000].to_pickle(f'data/seattle_lib_{ind}.pkl', compression='gzip')\n",
    "    \n",
    "    # print status/time\n",
    "    status_update(f'File {ind} out of 11 saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.iloc[40000000:50000000].to_pickle(f'data/seattle_lib_4b.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-b8306b2d38fe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-b8306b2d38fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    please break code\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "please break code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAVEYARD\n",
    "\n",
    "[[go back to the top](#Library-Usage-in-Seattle,-2005-2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # list of columns to keep\n",
    "# keep_cols = ['title', 'subjects', 'date', 'format_group', 'format_subgroup',\n",
    "#              'category_group', 'age_group']\n",
    "\n",
    "# # drop columns\n",
    "# df_merged = df_merged[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code_type == 'ItemType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'nadvd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'acdvd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.code_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_item = dd[dd.code_type == 'ItemType'][['code', 'description', 'format_group', 'format_subgroup', 'category_group', \n",
    "             'category_subgroup', 'age_group']]\n",
    "\n",
    "dd_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_item2 = dd[dd.code_type == 'ItemCollection'][['code', 'description', 'format_group', 'format_subgroup', 'category_group', \n",
    "             'category_subgroup', 'age_group']]\n",
    "\n",
    "dd_item2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.item_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_loc = dd[dd.code_type == 'Location'][['code', 'description']]\n",
    "\n",
    "dd_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.merge(dd_item, left_on='item_type', right_on='code')\n",
    "# test = test.merge(dd_loc, left_on='collection', right_on='code')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.format_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.collection.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = df.merge(dd_item2, left_on='Collection', right_on='code')\n",
    "\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('format_group').category_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.groupby('format_group').category_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[dd.code == 'nybot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.collection.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_loc.code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cod for cod in df.collection.unique() if cod in dd_loc.code.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Using the code below on 10 million rows is almost 39% faster and results in a saved file that is *nearly the same size as the original CSV file* (file is 23.22GB!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_merged.to_hdf('data/seattle_lib_temp_ten_mil__alt.hdf', 'mydata', format='table', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.title=='reader'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
